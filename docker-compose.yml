services:
  litellm-proxy:
    image: ghcr.io/berriai/litellm:main-stable
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    volumes:
      - ./litellm-config.yaml:/app/config.yaml
    env_file:
      - ./.env
    ports:
      - "4000:4000"


  openwebui-app:
    image: ghcr.io/open-webui/open-webui:main
    depends_on:
      - litellm-proxy
    environment:
      - WEBUI_AUTH=False
      - OPENAI_API_BASE_URL=http://litellm-proxy:4000/v1
      - OPENAI_API_KEY=sk-1234
      - ENABLE_RAG_WEB_SEARCH=False
      - ENABLE_RAG_LOCAL_WEB_FETCH=False
      - RAG_EMBEDDING_ENGINE=
      - SENTENCE_TRANSFORMERS_HOME=/tmp
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - open-webui:/app/backend/data
    ports:
      - "3000:8080"

  slack-bot:
    image: python:3.11-slim
    volumes:
      - ./slack-bot:/app
      - /var/run/docker.sock:/var/run/docker.sock
    working_dir: /app
    command: >
      sh -c "pip install slack_bolt requests &&
             python bot.py"
    env_file:
      - ./.env
    environment:
      - SESSION_MANAGER_URL=http://session-manager:8080

  session-manager:
    image: python:3.11-slim
    volumes:
      - ./session-manager:/app
      - /var/run/docker.sock:/var/run/docker.sock
      - session-data:/data
    working_dir: /app
    command: >
      sh -c "pip install fastapi uvicorn docker &&
             python manager.py"
    ports:
      - "8080:8080"
    environment:
      - DATA_DIR=/data

volumes:
  open-webui:
  session-data:
